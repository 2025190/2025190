import os
import argparse
import json
import cv2
import numpy as np
from pathlib import Path
from tqdm import tqdm
import shutil

def parse_args():
    parser = argparse.ArgumentParser(description='Prepare data for second stage model')
    parser.add_argument('--data-dir', type=str, default='runs/stage_two_data', help='Directory of data generated by first stage')
    parser.add_argument('--output-dir', type=str, default='runs/dataset_stage_two', help='Output directory for dataset')
    parser.add_argument('--image-size', type=int, default=224, help='Size of cropped and resized images')
    parser.add_argument('--original-dataset', type=str, default=None, help='Path to original dataset, used to find image files')
    return parser.parse_args()

def crop_and_resize(image, bbox, target_size):
    h, w = image.shape[:2]
    x1, y1, x2, y2 = int(bbox[0] * w), int(bbox[1] * h), int(bbox[2] * w), int(bbox[3] * h)
    x1, y1 = max(0, x1), max(0, y1)
    x2, y2 = min(w, x2), min(h, y2)
    cropped = image[y1:y2, x1:x2]
    if cropped.size > 0:
        crop_h, crop_w = cropped.shape[:2]
        max_side = max(crop_h, crop_w)
        square_img = np.zeros((max_side, max_side, 3), dtype=np.uint8)
        y_offset = (max_side - crop_h) // 2
        x_offset = (max_side - crop_w) // 2
        square_img[y_offset:y_offset+crop_h, x_offset:x_offset+crop_w] = cropped
        resized = cv2.resize(square_img, (target_size, target_size))
        return resized
    else:
        return np.zeros((target_size, target_size, 3), dtype=np.uint8)

def find_matching_gt(pred_bbox, gt_list, iou_threshold=0.5):
    max_iou = 0
    max_idx = None
    
    for idx, gt in enumerate(gt_list):
        gt_bbox = gt['bbox']
        iou = calculate_iou(pred_bbox, gt_bbox)
        
        if iou > max_iou:
            max_iou = iou
            max_idx = idx
    
    if max_iou >= iou_threshold:
        return max_idx
    return None

def calculate_iou(box1, box2):
    x1 = max(box1[0], box2[0])
    y1 = max(box1[1], box2[1])
    x2 = min(box1[2], box2[2])
    y2 = min(box1[3], box2[3])
    inter_area = max(0, x2 - x1) * max(0, y2 - y1)
    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])
    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])
    union_area = box1_area + box2_area - inter_area
    return inter_area / union_area if union_area > 0 else 0

def process_train_data(args, dataset_path, output_split_dir, mappings):
    print("Processing training set data...")
    images_dir = os.path.join(output_split_dir, 'images')
    os.makedirs(images_dir, exist_ok=True)
    labels_dir = os.path.join(output_split_dir, 'labels')
    os.makedirs(labels_dir, exist_ok=True)
    results_dir = os.path.join(dataset_path, 'results')
    result_files = [f for f in os.listdir(results_dir) if f.endswith('.json')]
    stats = {
        'total_images': 0,
        'total_gt_patches': 0,
        'missed_detections': 0,
    }
    original_images_dir = None
    if args.original_dataset:
        dataset_root = Path(args.original_dataset)
        if dataset_root.exists():
            train_images_path = dataset_root / 'train' / 'images'
            if train_images_path.exists():
                original_images_dir = str(train_images_path)
                print(f"Found original training images directory: {original_images_dir}")
    
    if not original_images_dir:
        print("Warning: Original images directory not found, please use --original-dataset to specify the original dataset path")
        return
    for result_file in tqdm(result_files):
        result_path = os.path.join(results_dir, result_file)
        with open(result_path, 'r') as f:
            result_data = json.load(f)
        image_file = result_data['image_file']
        image_path = os.path.join(original_images_dir, image_file)
        if not os.path.exists(image_path):
            print(f"Warning: Image file does not exist: {image_path}")
            continue
        image = cv2.imread(image_path)
        if image is None:
            print(f"Warning: Unable to read image file: {image_path}")
            continue
        original_resized = cv2.resize(image, (args.image_size, args.image_size))
        base_name = os.path.splitext(image_file)[0]
        original_image_path = os.path.join(images_dir, f"{base_name}_original.jpg")
        cv2.imwrite(original_image_path, original_resized)
        video_name = result_data['video_name']
        frame_id = result_data['frame_id']
        ground_truths = result_data['ground_truths']
        predictions = result_data['predictions']
        original_label = {
            'video_name': video_name,
            'frame_id': frame_id,
            'image_type': 'original',
            'gt_patches': len(ground_truths)
        }
        gt_bboxes = []
        gt_tool_ids = []
        gt_action_ids = []
        gt_target_ids = []
        gt_triplet_ids = []
        
        for gt in ground_truths:
            gt_bboxes.append(gt['bbox'])
            gt_tool_ids.append(gt['tool_id'])
            gt_action_ids.append(gt['action_id'])
            gt_target_ids.append(gt['target_id'])
            gt_triplet_ids.append(gt['triplet_id'])
        original_label['ground_truths'] = ground_truths
        
        original_label_path = os.path.join(labels_dir, f"{base_name}_original.json")
        with open(original_label_path, 'w') as f:
            json.dump(original_label, f, indent=2)
        
        stats['total_images'] += 1
        for gt_idx, gt in enumerate(ground_truths):
            gt_bbox = gt['bbox']
            gt_patch = crop_and_resize(image, gt_bbox, args.image_size)
            gt_patch_path = os.path.join(images_dir, f"{base_name}_gt_{gt_idx}.jpg")
            cv2.imwrite(gt_patch_path, gt_patch)
            matching_pred = None
            all_class_probs = None
            
            for pred in predictions:
                pred_bbox = pred['bbox']
                iou = calculate_iou(gt_bbox, pred_bbox)
                
                if iou >= 0.5:
                    matching_pred = pred
                    if 'all_class_probs' in pred:
                        all_class_probs = pred['all_class_probs']
                    break
            if matching_pred is None:
                num_classes = len(mappings.get('tool_names', {}))
                if num_classes == 0 and len(predictions) > 0 and 'all_class_probs' in predictions[0]:
                    num_classes = len(predictions[0]['all_class_probs'])
                all_class_probs = [0.0] * num_classes
                tool_id = gt['tool_id']
                if tool_id < num_classes:
                    all_class_probs[tool_id] = 1.0
                
                stats['missed_detections'] += 1
            gt_patch_label = {
                'video_name': video_name,
                'frame_id': frame_id,
                'image_type': 'gt_patch',
                'gt_bbox': gt_bbox,
                'triplet_id': gt['triplet_id'],
                'tool_id': gt['tool_id'],
                'action_id': gt['action_id'],
                'target_id': gt['target_id'],
                'all_class_probs': all_class_probs
            }
            
            gt_patch_label_path = os.path.join(labels_dir, f"{base_name}_gt_{gt_idx}.json")
            with open(gt_patch_label_path, 'w') as f:
                json.dump(gt_patch_label, f, indent=2)
            
            stats['total_gt_patches'] += 1
    print(f"Training set processing completed:")
    print(f"- Total images: {stats['total_images']}")
    print(f"- Total GT patches: {stats['total_gt_patches']}")
    print(f"- Missed detections: {stats['missed_detections']}")

def process_val_test_data(args, dataset_path, output_split_dir, split, mappings):
    print(f"Processing {split} set data...")
    images_dir = os.path.join(output_split_dir, 'images')
    os.makedirs(images_dir, exist_ok=True)
    labels_dir = os.path.join(output_split_dir, 'labels')
    os.makedirs(labels_dir, exist_ok=True)
    results_dir = os.path.join(dataset_path, 'results')
    result_files = [f for f in os.listdir(results_dir) if f.endswith('.json')]
    stats = {
        'total_images': 0,
        'total_pred_patches': 0,
        'correct_detections': 0,
        'false_positives': 0,
    }
    original_images_dir = None
    if args.original_dataset:
        dataset_root = Path(args.original_dataset)
        if dataset_root.exists():
            images_path = dataset_root / split / 'images'
            if images_path.exists():
                original_images_dir = str(images_path)
                print(f"Found original {split} images directory: {original_images_dir}")
    
    if not original_images_dir:
        print(f"Warning: Original {split} images directory not found, please use --original-dataset to specify the original dataset path")
        return
    for result_file in tqdm(result_files):
        result_path = os.path.join(results_dir, result_file)
        with open(result_path, 'r') as f:
            result_data = json.load(f)
        image_file = result_data['image_file']
        image_path = os.path.join(original_images_dir, image_file)
        if not os.path.exists(image_path):
            print(f"Warning: Image file does not exist: {image_path}")
            continue
        image = cv2.imread(image_path)
        if image is None:
            print(f"Warning: Unable to read image file: {image_path}")
            continue
        original_resized = cv2.resize(image, (args.image_size, args.image_size))
        base_name = os.path.splitext(image_file)[0]
        original_image_path = os.path.join(images_dir, f"{base_name}_original.jpg")
        cv2.imwrite(original_image_path, original_resized)
        video_name = result_data['video_name']
        frame_id = result_data['frame_id']
        ground_truths = result_data['ground_truths']
        predictions = result_data['predictions']
        original_label = {
            'video_name': video_name,
            'frame_id': frame_id,
            'image_type': 'original',
            'pred_patches': len(predictions)
        }
        gt_bboxes = []
        gt_tool_ids = []
        gt_action_ids = []
        gt_target_ids = []
        gt_triplet_ids = []
        
        for gt in ground_truths:
            gt_bboxes.append(gt['bbox'])
            gt_tool_ids.append(gt['tool_id'])
            gt_action_ids.append(gt['action_id'])
            gt_target_ids.append(gt['target_id'])
            gt_triplet_ids.append(gt['triplet_id'])
        original_label['ground_truths'] = ground_truths
        
        original_label_path = os.path.join(labels_dir, f"{base_name}_original.json")
        with open(original_label_path, 'w') as f:
            json.dump(original_label, f, indent=2)
        
        stats['total_images'] += 1
        for pred_idx, pred in enumerate(predictions):
            pred_bbox = pred['bbox']
            pred_patch = crop_and_resize(image, pred_bbox, args.image_size)
            pred_patch_path = os.path.join(images_dir, f"{base_name}_pred_{pred_idx}.jpg")
            cv2.imwrite(pred_patch_path, pred_patch)
            score = pred['score']
            tool_id = pred['tool_id']
            all_class_probs = pred.get('all_class_probs', None)
            matching_gt_idx = find_matching_gt(pred_bbox, ground_truths)
            matching_gt = ground_truths[matching_gt_idx] if matching_gt_idx is not None else None
            pred_patch_label = {
                'video_name': video_name,
                'frame_id': frame_id,
                'image_type': 'pred_patch',
                'pred_bbox': pred_bbox,
                'pred_score': score,
                'pred_tool_id': tool_id,
                'all_class_probs': all_class_probs
            }
            if matching_gt:
                pred_patch_label.update({
                    'gt_bbox': matching_gt['bbox'],
                    'gt_triplet_id': matching_gt['triplet_id'],
                    'gt_tool_id': matching_gt['tool_id'],
                    'gt_action_id': matching_gt['action_id'],
                    'gt_target_id': matching_gt['target_id']
                })
                stats['correct_detections'] += 1
            else:
                pred_patch_label.update({
                    'gt_bbox': None,
                    'gt_triplet_id': None,
                    'gt_tool_id': None,
                    'gt_action_id': None,
                    'gt_target_id': None
                })
                stats['false_positives'] += 1
            
            pred_patch_label_path = os.path.join(labels_dir, f"{base_name}_pred_{pred_idx}.json")
            with open(pred_patch_label_path, 'w') as f:
                json.dump(pred_patch_label, f, indent=2)
            
            stats['total_pred_patches'] += 1
    print(f"{split} set processing completed:")
    print(f"- Total images: {stats['total_images']}")
    print(f"- Total predicted patches: {stats['total_pred_patches']}")
    print(f"- Correct detections: {stats['correct_detections']}")
    print(f"- False positives: {stats['false_positives']}")

def main():
    args = parse_args()
    data_dir = Path(args.data_dir)
    if not data_dir.exists():
        print(f"Error: Data directory does not exist: {data_dir}")
        return
    output_dir = Path(args.output_dir)
    os.makedirs(output_dir, exist_ok=True)
    mappings = {}
    mappings_file = os.path.join(data_dir, 'class_mappings.json')
    if os.path.exists(mappings_file):
        with open(mappings_file, 'r') as f:
            mappings = json.load(f)
        shutil.copy(mappings_file, os.path.join(output_dir, 'class_mappings.json'))
    else:
        print(f"Warning: Class mapping file not found: {mappings_file}")
    for split in ['train', 'val', 'test']:
        split_dir = os.path.join(data_dir, split)
        if os.path.exists(split_dir):
            output_split_dir = os.path.join(output_dir, split)
            os.makedirs(output_split_dir, exist_ok=True)
            if split == 'train':
                process_train_data(args, split_dir, output_split_dir, mappings)
            else:
                process_val_test_data(args, split_dir, output_split_dir, split, mappings)
        else:
            print(f"Warning: {split} data directory not found: {split_dir}")
    
    print(f"Second stage data processing completed, output saved in: {output_dir}")

if __name__ == '__main__':
    main()